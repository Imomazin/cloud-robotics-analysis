{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MjuVbrG-_oyQXex6Ui6e4-HCZhNc1pA3","timestamp":1743202224987},{"file_id":"1ULOydRp5Z6XaxjsNJySRvtLewur4QkGm","timestamp":1743202015456},{"file_id":"16NDQqkawsNFH61SFXHgir5TkdoG2T5jk","timestamp":1743201985611}],"authorship_tag":"ABX9TyNUpxppfdLSOTDh7pWZ42L1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LhkCmX-bVArR","executionInfo":{"status":"ok","timestamp":1743193197246,"user_tz":0,"elapsed":176,"user":{"displayName":"Imo Enang","userId":"02918714309836997024"}},"outputId":"a99ded24-101e-4d6e-b3d2-14f5325e90a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Cloud Robotics Analysis - Part 1 (Data Generation and Analysis)...\n","Loading and preprocessing data...\n","Generating sample data...\n","Analyzing performance metrics...\n","Average performance data saved to 'results/average_performance_by_algorithm.csv'\n","Improvement percentages saved to 'results/improvement_percentages.csv'\n","Running statistical significance tests...\n","Statistical test results saved to 'results/statistical_test_results.csv'\n","Correlation results saved to 'results/performance_metric_correlations.csv'\n","\n","Summary of Performance Analysis:\n","Number of algorithm types analyzed: 5\n","\n","Average Performance by Algorithm Type:\n","      algorithm_category  task_completion_time_ms  \\\n","0      Centralized Cloud                   1850.0   \n","1  Conventional (Non-AI)                   2340.0   \n","2       Distributed Edge                    950.0   \n","3     Federated Learning                   1250.0   \n","4      Hybrid Cloud-Edge                    780.0   \n","\n","   resource_utilization_percent  \n","0                          78.4  \n","1                          54.8  \n","2                          65.2  \n","3                          72.6  \n","4                          81.3  \n","\n","Percentage Improvements over Conventional:\n","   algorithm_category  task_completion_time_improvement  \\\n","0   Centralized Cloud                         20.940171   \n","1    Distributed Edge                         59.401709   \n","2  Federated Learning                         46.581197   \n","3   Hybrid Cloud-Edge                         66.666667   \n","\n","   resource_utilization_improvement  \n","0                         43.065693  \n","1                         18.978102  \n","2                         32.481752  \n","3                         48.357664  \n","\n","Results saved for part 2 processing\n","Analyzing implementation barriers...\n","Barriers by impact saved to 'results/barriers_by_impact.csv'\n","Barrier category summary saved to 'results/barrier_category_summary.csv'\n","Top co-occurring barriers saved to 'results/top_co_occurring_barriers.csv'\n","\n","Summary of Barrier Analysis:\n","Number of barriers analyzed: 80\n","\n","Top 5 Implementation Barriers by Impact:\n","               barrier_name barrier_category  impact_score\n","0   System interoperability        Technical         4.048\n","8   System interoperability        Technical         4.048\n","71  System interoperability        Technical         4.048\n","46  System interoperability        Technical         4.048\n","32  System interoperability        Technical         4.048\n","\n","Average Impact by Category:\n","  barrier_category  impact_score\n","0       Contextual      2.258667\n","1   Organizational      2.731200\n","2        Technical      3.275375\n","\n","Results saved for part 2 processing\n","\n","Part 1 analysis complete! Results saved to 'results' directory.\n","You can now run Part 2 to generate visualizations and the implementation framework.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as stats\n","from pathlib import Path\n","import os\n","\n","# Create output directory for results\n","os.makedirs(\"results\", exist_ok=True)\n","\n","# -----------------------------------------------\n","# Generate Sample Data\n","# -----------------------------------------------\n","\n","def generate_sample_data():\n","    \"\"\"\n","    Generate sample data for analysis\n","    \"\"\"\n","    print(\"Generating sample data...\")\n","\n","    # IEEE Dataset - Sample performance metrics\n","    ieee_data = pd.DataFrame({\n","        'algorithm_name': ['Conventional', 'Cloud-Based', 'Edge-Based', 'Federated', 'Hybrid'],\n","        'task_completion_time': [2412, 1925, 982, 1278, 802],\n","        'time_unit': ['ms', 'ms', 'ms', 'ms', 'ms'],\n","        'resource_utilization_percent': [53.2, 76.8, 63.7, 71.9, 79.6],\n","        'energy_efficiency': [2.4, 3.1, 4.0, 3.8, 4.4],\n","        'failure_rate_percent': [7.5, 4.9, 2.4, 3.2, 1.9],\n","        'adaptation_time': [678, 435, 194, 226, 165],\n","        'adaptation_unit': ['ms', 'ms', 'ms', 'ms', 'ms']\n","    })\n","\n","    # Kaggle Dataset - Sample performance metrics\n","    kaggle_data = pd.DataFrame({\n","        'algorithm_type': ['Traditional', 'Centralized', 'Distributed', 'Federated', 'Hybrid'],\n","        'completion_time': [2268, 1775, 918, 1222, 758],\n","        'time_unit': ['ms', 'ms', 'ms', 'ms', 'ms'],\n","        'resource_usage_percent': [56.4, 80.0, 66.7, 73.3, 83.0],\n","        'energy_consumption': [3.6, 3.0, 2.4, 2.5, 2.2],  # Lower is better\n","        'error_rate_percent': [6.9, 4.5, 2.2, 3.0, 1.7],\n","        'adaptation_time': [622, 405, 166, 194, 135],\n","        'adaptation_unit': ['ms', 'ms', 'ms', 'ms', 'ms']\n","    })\n","\n","    # Wan et al. Dataset - Sample performance metrics\n","    wan_data = pd.DataFrame({\n","        'algorithm': ['Non-AI', 'Cloud', 'Edge', 'Federated', 'Hybrid'],\n","        'task_time': [2340, 1850, 950, 1250, 780],\n","        'time_unit': ['ms', 'ms', 'ms', 'ms', 'ms'],\n","        'resource_util_percent': [54.8, 78.4, 65.2, 72.6, 81.3],\n","        'energy_eff': [2.6, 3.2, 4.1, 3.9, 4.5],\n","        'failure_percent': [7.2, 4.7, 2.3, 3.1, 1.8],\n","        'adapt_time': [650, 420, 180, 210, 150],\n","        'adapt_unit': ['ms', 'ms', 'ms', 'ms', 'ms']\n","    })\n","\n","    # Chaari et al. Dataset - Sample implementation barriers\n","    barriers = [\n","        {\"barrier_name\": \"System interoperability\", \"barrier_category\": \"Technical\", \"frequency_percent\": 92, \"severity_rating\": 4.4},\n","        {\"barrier_name\": \"Network latency\", \"barrier_category\": \"Technical\", \"frequency_percent\": 87, \"severity_rating\": 4.2},\n","        {\"barrier_name\": \"Data security concerns\", \"barrier_category\": \"Technical\", \"frequency_percent\": 76, \"severity_rating\": 4.7},\n","        {\"barrier_name\": \"Implementation costs\", \"barrier_category\": \"Organizational\", \"frequency_percent\": 81, \"severity_rating\": 4.1},\n","        {\"barrier_name\": \"Expertise requirements\", \"barrier_category\": \"Organizational\", \"frequency_percent\": 65, \"severity_rating\": 3.9},\n","        {\"barrier_name\": \"Industry-specific requirements\", \"barrier_category\": \"Contextual\", \"frequency_percent\": 62, \"severity_rating\": 3.8},\n","        {\"barrier_name\": \"Regulatory compliance\", \"barrier_category\": \"Contextual\", \"frequency_percent\": 48, \"severity_rating\": 4.3},\n","        {\"barrier_name\": \"Organizational resistance\", \"barrier_category\": \"Organizational\", \"frequency_percent\": 54, \"severity_rating\": 3.6},\n","        {\"barrier_name\": \"Legacy system integration\", \"barrier_category\": \"Technical\", \"frequency_percent\": 58, \"severity_rating\": 3.4},\n","        {\"barrier_name\": \"Data volume management\", \"barrier_category\": \"Technical\", \"frequency_percent\": 51, \"severity_rating\": 3.3}\n","    ]\n","\n","    chaari_data = pd.DataFrame(barriers)\n","\n","    # Add implementation IDs and present flags for co-occurrence analysis\n","    implementations = []\n","    for i in range(10):\n","        for barrier in barriers[:6]:  # First 6 barriers present in all implementations\n","            implementations.append({\n","                \"implementation_id\": i,\n","                \"barrier_name\": barrier[\"barrier_name\"],\n","                \"barrier_category\": barrier[\"barrier_category\"],\n","                \"frequency_percent\": barrier[\"frequency_percent\"],\n","                \"severity_rating\": barrier[\"severity_rating\"],\n","                \"present\": 1\n","            })\n","\n","        # Randomly add some of the remaining barriers\n","        for barrier in barriers[6:]:\n","            if np.random.random() > 0.5:  # 50% chance of barrier being present\n","                implementations.append({\n","                    \"implementation_id\": i,\n","                    \"barrier_name\": barrier[\"barrier_name\"],\n","                    \"barrier_category\": barrier[\"barrier_category\"],\n","                    \"frequency_percent\": barrier[\"frequency_percent\"],\n","                    \"severity_rating\": barrier[\"severity_rating\"],\n","                    \"present\": 1\n","                })\n","\n","    chaari_data_expanded = pd.DataFrame(implementations)\n","\n","    return ieee_data, kaggle_data, wan_data, chaari_data_expanded\n","\n","# -----------------------------------------------\n","# Data Preprocessing Functions\n","# -----------------------------------------------\n","\n","def standardize_time(df, col_name, unit_col):\n","    \"\"\"\n","    Standardize time measurements to milliseconds\n","    \"\"\"\n","    df_copy = df.copy()\n","\n","    if unit_col in df_copy.columns:\n","        # Convert seconds to milliseconds\n","        mask_seconds = df_copy[unit_col] == 's'\n","        df_copy.loc[mask_seconds, col_name] = df_copy.loc[mask_seconds, col_name] * 1000\n","\n","        # Convert microseconds to milliseconds\n","        mask_microseconds = df_copy[unit_col] == 'μs'\n","        df_copy.loc[mask_microseconds, col_name] = df_copy.loc[mask_microseconds, col_name] / 1000\n","\n","    # Rename column to indicate milliseconds\n","    new_col_name = f\"{col_name}_ms\" if not col_name.endswith('_ms') else col_name\n","    df_copy.rename(columns={col_name: new_col_name}, inplace=True)\n","\n","    return df_copy\n","\n","def categorize_algorithm(algorithm_name):\n","    \"\"\"\n","    Categorize algorithm names into standardized categories\n","    \"\"\"\n","    algorithm_name = str(algorithm_name).lower()\n","\n","    if any(term in algorithm_name for term in ['traditional', 'conventional', 'baseline', 'non-ai', 'non ai']):\n","        return 'Conventional (Non-AI)'\n","    elif any(term in algorithm_name for term in ['central', 'cloud-only', 'cloud only', 'remote', 'cloud']):\n","        return 'Centralized Cloud'\n","    elif any(term in algorithm_name for term in ['distrib', 'edge', 'local']):\n","        return 'Distributed Edge'\n","    elif any(term in algorithm_name for term in ['federated', 'collaborative', 'collective']):\n","        return 'Federated Learning'\n","    elif any(term in algorithm_name for term in ['hybrid', 'adaptive', 'multi-tier']):\n","        return 'Hybrid Cloud-Edge'\n","    else:\n","        return 'Other'\n","\n","def standardize_column_names(df):\n","    \"\"\"\n","    Standardize column names across datasets\n","    \"\"\"\n","    column_mapping = {\n","        'task_time_ms': 'task_completion_time_ms',\n","        'completion_time_ms': 'task_completion_time_ms',\n","        'resource_util_percent': 'resource_utilization_percent',\n","        'resource_usage_percent': 'resource_utilization_percent',\n","        'energy_eff': 'energy_efficiency_score',\n","        'energy_efficiency': 'energy_efficiency_score',\n","        'energy_consumption': 'energy_efficiency_score',\n","        'failure_percent': 'failure_rate_percent',\n","        'error_rate_percent': 'failure_rate_percent',\n","        'adapt_time_ms': 'adaptation_time_ms'\n","    }\n","\n","    df_copy = df.copy()\n","\n","    for old_col, new_col in column_mapping.items():\n","        if old_col in df_copy.columns:\n","            df_copy.rename(columns={old_col: new_col}, inplace=True)\n","\n","    return df_copy\n","\n","def normalize_efficiency(df, col_name, target_scale=5.0):\n","    \"\"\"\n","    Normalize efficiency scores to 0-5 scale\n","    \"\"\"\n","    df_copy = df.copy()\n","\n","    # If column contains energy consumption (where lower is better)\n","    if 'consumption' in col_name:\n","        # Find the minimum value (best performance)\n","        min_val = df_copy[col_name].min()\n","        # Invert the scale so lower consumption = higher score\n","        df_copy[col_name] = target_scale * (1 - (df_copy[col_name] - min_val) /\n","                                           (df_copy[col_name].max() - min_val))\n","    else:\n","        # For scores where higher is better, simply normalize to 0-5 scale\n","        max_val = df_copy[col_name].max()\n","        df_copy[col_name] = df_copy[col_name] / max_val * target_scale\n","\n","    return df_copy\n","\n","def process_barriers_data(df):\n","    \"\"\"\n","    Process and analyze implementation barriers data\n","    \"\"\"\n","    # Calculate impact scores (frequency × severity)\n","    df['impact_score'] = df['frequency_percent'] * df['severity_rating'] / 100\n","\n","    # Sort by impact score\n","    return df.sort_values('impact_score', ascending=False)\n","\n","def load_and_preprocess_data():\n","    \"\"\"\n","    Load data from all sources and preprocess for analysis\n","    \"\"\"\n","    print(\"Loading and preprocessing data...\")\n","\n","    # Generate sample data\n","    ieee_data, kaggle_data, wan_data, chaari_data = generate_sample_data()\n","\n","    # Standardize time measurements to milliseconds\n","    ieee_data = standardize_time(ieee_data, 'task_completion_time', 'time_unit')\n","    ieee_data = standardize_time(ieee_data, 'adaptation_time', 'adaptation_unit')\n","\n","    kaggle_data = standardize_time(kaggle_data, 'completion_time', 'time_unit')\n","    kaggle_data = standardize_time(kaggle_data, 'adaptation_time', 'adaptation_unit')\n","\n","    wan_data = standardize_time(wan_data, 'task_time', 'time_unit')\n","    wan_data = standardize_time(wan_data, 'adapt_time', 'adapt_unit')\n","\n","    # Standardize algorithm categories\n","    ieee_data['algorithm_category'] = ieee_data['algorithm_name'].apply(categorize_algorithm)\n","    kaggle_data['algorithm_category'] = kaggle_data['algorithm_type'].apply(categorize_algorithm)\n","    wan_data['algorithm_category'] = wan_data['algorithm'].apply(categorize_algorithm)\n","\n","    # Standardize column names\n","    ieee_data = standardize_column_names(ieee_data)\n","    kaggle_data = standardize_column_names(kaggle_data)\n","    wan_data = standardize_column_names(wan_data)\n","\n","    # Normalize energy efficiency scores to 0-5 scale\n","    ieee_data = normalize_efficiency(ieee_data, 'energy_efficiency_score')\n","    kaggle_data = normalize_efficiency(kaggle_data, 'energy_efficiency_score')\n","    wan_data = normalize_efficiency(wan_data, 'energy_efficiency_score')\n","\n","    # Create integrated dataset\n","    integrated_data = pd.DataFrame()\n","\n","    # Process each dataset and add to integrated data\n","    for dataset, source_name in zip([ieee_data, kaggle_data, wan_data],\n","                                    ['IEEE', 'Kaggle', 'Wan et al.']):\n","        # Calculate averages by algorithm category\n","        avg_by_alg = dataset.groupby('algorithm_category').agg({\n","            'task_completion_time_ms': 'mean',\n","            'resource_utilization_percent': 'mean',\n","            'energy_efficiency_score': 'mean',\n","            'failure_rate_percent': 'mean',\n","            'adaptation_time_ms': 'mean'\n","        }).reset_index()\n","\n","        # Add source column\n","        avg_by_alg['source'] = source_name\n","\n","        # Append to integrated data\n","        integrated_data = pd.concat([integrated_data, avg_by_alg], ignore_index=True)\n","\n","    # Process barriers data\n","    barriers_data = process_barriers_data(chaari_data)\n","\n","    return integrated_data, barriers_data\n","\n","# -----------------------------------------------\n","# Performance Analysis Functions\n","# -----------------------------------------------\n","\n","def calculate_improvements(data):\n","    \"\"\"\n","    Calculate percentage improvements relative to conventional approaches\n","    \"\"\"\n","    # Get baseline (conventional) values\n","    baseline = data[data['algorithm_category'] == 'Conventional (Non-AI)'].iloc[0]\n","\n","    results = []\n","    for _, row in data.iterrows():\n","        if row['algorithm_category'] == 'Conventional (Non-AI)':\n","            continue\n","\n","        improvement = {\n","            'algorithm_category': row['algorithm_category'],\n","            'task_completion_time_improvement': (baseline['task_completion_time_ms'] - row['task_completion_time_ms']) / baseline['task_completion_time_ms'] * 100,\n","            'resource_utilization_improvement': (row['resource_utilization_percent'] - baseline['resource_utilization_percent']) / baseline['resource_utilization_percent'] * 100,\n","            'energy_efficiency_improvement': (row['energy_efficiency_score'] - baseline['energy_efficiency_score']) / baseline['energy_efficiency_score'] * 100,\n","            'failure_reduction': (baseline['failure_rate_percent'] - row['failure_rate_percent']) / baseline['failure_rate_percent'] * 100,\n","            'adaptation_time_improvement': (baseline['adaptation_time_ms'] - row['adaptation_time_ms']) / baseline['adaptation_time_ms'] * 100\n","        }\n","        results.append(improvement)\n","\n","    return pd.DataFrame(results)\n","\n","def run_statistical_tests(data):\n","    \"\"\"\n","    Run paired t-tests to assess statistical significance of differences\n","    \"\"\"\n","    print(\"Running statistical significance tests...\")\n","\n","    # Group data by algorithm category\n","    grouped = data.groupby('algorithm_category')\n","\n","    # Dictionary to store test results\n","    results = {}\n","\n","    # Metrics to test\n","    metrics = ['task_completion_time_ms', 'resource_utilization_percent',\n","               'energy_efficiency_score', 'failure_rate_percent', 'adaptation_time_ms']\n","\n","    try:\n","        # Get data for each algorithm type\n","        conventional = grouped.get_group('Conventional (Non-AI)')\n","        centralized = grouped.get_group('Centralized Cloud')\n","        distributed = grouped.get_group('Distributed Edge')\n","        federated = grouped.get_group('Federated Learning')\n","        hybrid = grouped.get_group('Hybrid Cloud-Edge')\n","\n","        # Run t-tests for key comparisons\n","        for metric in metrics:\n","            # Hybrid vs Conventional\n","            t_stat, p_val = stats.ttest_ind(\n","                hybrid[metric].dropna(),\n","                conventional[metric].dropna()\n","            )\n","            results[f'hybrid_vs_conventional_{metric}'] = {'t_stat': t_stat, 'p_value': p_val}\n","\n","            # Hybrid vs Centralized\n","            t_stat, p_val = stats.ttest_ind(\n","                hybrid[metric].dropna(),\n","                centralized[metric].dropna()\n","            )\n","            results[f'hybrid_vs_centralized_{metric}'] = {'t_stat': t_stat, 'p_value': p_val}\n","\n","            # Hybrid vs Distributed\n","            t_stat, p_val = stats.ttest_ind(\n","                hybrid[metric].dropna(),\n","                distributed[metric].dropna()\n","            )\n","            results[f'hybrid_vs_distributed_{metric}'] = {'t_stat': t_stat, 'p_value': p_val}\n","    except KeyError as e:\n","        print(f\"Warning: Missing algorithm category: {e}\")\n","        return None\n","\n","    # Convert to DataFrame for easier viewing\n","    results_df = pd.DataFrame({\n","        'comparison': list(results.keys()),\n","        't_statistic': [v['t_stat'] for v in results.values()],\n","        'p_value': [v['p_value'] for v in results.values()],\n","        'significant': [v['p_value'] < 0.05 for v in results.values()]\n","    })\n","\n","    results_df.to_csv('results/statistical_test_results.csv', index=False)\n","    print(f\"Statistical test results saved to 'results/statistical_test_results.csv'\")\n","\n","    return results_df\n","\n","def calculate_correlations(data):\n","    \"\"\"\n","    Calculate correlations between different performance metrics\n","    \"\"\"\n","    # Select only numeric columns for correlation\n","    numeric_cols = ['task_completion_time_ms', 'resource_utilization_percent',\n","                   'energy_efficiency_score', 'failure_rate_percent', 'adaptation_time_ms']\n","\n","    # Calculate correlation matrix\n","    correlation_matrix = data[numeric_cols].corr()\n","\n","    return correlation_matrix\n","\n","def analyze_performance(data):\n","    \"\"\"\n","    Analyze performance metrics across algorithm types\n","    \"\"\"\n","    print(\"Analyzing performance metrics...\")\n","\n","    # Calculate overall averages by algorithm type\n","    avg_performance = data.groupby('algorithm_category').agg({\n","        'task_completion_time_ms': 'mean',\n","        'resource_utilization_percent': 'mean',\n","        'energy_efficiency_score': 'mean',\n","        'failure_rate_percent': 'mean',\n","        'adaptation_time_ms': 'mean'\n","    }).reset_index()\n","\n","    # Save raw performance data\n","    avg_performance.to_csv('results/average_performance_by_algorithm.csv', index=False)\n","    print(f\"Average performance data saved to 'results/average_performance_by_algorithm.csv'\")\n","\n","    # Calculate percentage improvements over conventional\n","    improvements = calculate_improvements(avg_performance)\n","    improvements.to_csv('results/improvement_percentages.csv', index=False)\n","    print(f\"Improvement percentages saved to 'results/improvement_percentages.csv'\")\n","\n","    # Run statistical significance tests\n","    stat_results = run_statistical_tests(data)\n","\n","    # Calculate correlations between metrics\n","    corr_results = calculate_correlations(data)\n","    corr_results.to_csv('results/performance_metric_correlations.csv')\n","    print(f\"Correlation results saved to 'results/performance_metric_correlations.csv'\")\n","\n","    # Print summary of findings\n","    print(\"\\nSummary of Performance Analysis:\")\n","    print(f\"Number of algorithm types analyzed: {len(avg_performance)}\")\n","    print(\"\\nAverage Performance by Algorithm Type:\")\n","    print(avg_performance[['algorithm_category', 'task_completion_time_ms', 'resource_utilization_percent']])\n","    print(\"\\nPercentage Improvements over Conventional:\")\n","    print(improvements[['algorithm_category', 'task_completion_time_improvement', 'resource_utilization_improvement']])\n","\n","    # Save results to file for part 2\n","    results = {\n","        'avg_performance': avg_performance,\n","        'improvements': improvements,\n","        'stat_results': stat_results,\n","        'corr_results': corr_results\n","    }\n","\n","    pd.to_pickle(results, 'results/performance_analysis_results.pkl')\n","    print(\"\\nResults saved for part 2 processing\")\n","\n","    return avg_performance, improvements, stat_results, corr_results\n","\n","# -----------------------------------------------\n","# Barriers Analysis Functions\n","# -----------------------------------------------\n","\n","def analyze_barriers(barriers_data):\n","    \"\"\"\n","    Analyze implementation barriers data\n","    \"\"\"\n","    print(\"Analyzing implementation barriers...\")\n","\n","    # Save sorted barriers by impact\n","    barriers_data.to_csv('results/barriers_by_impact.csv', index=False)\n","    print(f\"Barriers by impact saved to 'results/barriers_by_impact.csv'\")\n","\n","    # Aggregate barriers by category\n","    category_summary = barriers_data.groupby('barrier_category').agg({\n","        'impact_score': 'mean',\n","        'frequency_percent': 'mean',\n","        'severity_rating': 'mean'\n","    }).reset_index()\n","\n","    category_summary.to_csv('results/barrier_category_summary.csv', index=False)\n","    print(f\"Barrier category summary saved to 'results/barrier_category_summary.csv'\")\n","\n","    # Analyze barrier co-occurrence if data is available\n","    if 'implementation_id' in barriers_data.columns and 'present' in barriers_data.columns:\n","        # Create a matrix of 1/0 values for each implementation and barrier\n","        pivot_data = pd.pivot_table(\n","            barriers_data,\n","            values='present',\n","            index='implementation_id',\n","            columns='barrier_name',\n","            aggfunc='max',\n","            fill_value=0\n","        )\n","\n","        # Calculate co-occurrence matrix\n","        co_occurrence = pivot_data.T.dot(pivot_data)\n","\n","        # Normalize by the number of occurrences\n","        for i in range(len(co_occurrence)):\n","            co_occurrence.iloc[i, i] = 0  # Set diagonal to zero\n","\n","        # Find most common co-occurring barriers\n","        barrier_pairs = []\n","        for i in range(len(co_occurrence)):\n","            for j in range(i+1, len(co_occurrence)):\n","                barrier_pairs.append({\n","                    'barrier1': co_occurrence.index[i],\n","                    'barrier2': co_occurrence.index[j],\n","                    'co_occurrences': co_occurrence.iloc[i, j]\n","                })\n","\n","        barrier_pair_df = pd.DataFrame(barrier_pairs)\n","        top_co_occurring = barrier_pair_df.sort_values('co_occurrences', ascending=False).head(10)\n","\n","        top_co_occurring.to_csv('results/top_co_occurring_barriers.csv', index=False)\n","        print(f\"Top co-occurring barriers saved to 'results/top_co_occurring_barriers.csv'\")\n","\n","    # Print summary of findings\n","    print(\"\\nSummary of Barrier Analysis:\")\n","    print(f\"Number of barriers analyzed: {len(barriers_data)}\")\n","    print(\"\\nTop 5 Implementation Barriers by Impact:\")\n","    print(barriers_data[['barrier_name', 'barrier_category', 'impact_score']].head(5))\n","    print(\"\\nAverage Impact by Category:\")\n","    print(category_summary[['barrier_category', 'impact_score']])\n","\n","    # Save results to file for part 2\n","    results = {\n","        'barriers_by_impact': barriers_data,\n","        'category_summary': category_summary\n","    }\n","\n","    pd.to_pickle(results, 'results/barriers_analysis_results.pkl')\n","    print(\"\\nResults saved for part 2 processing\")\n","\n","    return barriers_data, category_summary\n","\n","# -----------------------------------------------\n","# Main Execution for Part 1\n","# -----------------------------------------------\n","\n","def main_part1():\n","    \"\"\"\n","    Main execution function for part 1\n","    \"\"\"\n","    print(\"Starting Cloud Robotics Analysis - Part 1 (Data Generation and Analysis)...\")\n","\n","    # Load and preprocess data\n","    integrated_data, barriers_data = load_and_preprocess_data()\n","\n","    # Analyze performance metrics\n","    avg_performance, improvements, stat_results, corr_results = analyze_performance(integrated_data)\n","\n","    # Analyze implementation barriers\n","    barriers_summary, category_summary = analyze_barriers(barriers_data)\n","\n","    print(\"\\nPart 1 analysis complete! Results saved to 'results' directory.\")\n","    print(\"You can now run Part 2 to generate visualizations and the implementation framework.\")\n","\n","    return {\n","        'performance': {\n","            'average': avg_performance,\n","            'improvements': improvements,\n","            'statistical_tests': stat_results,\n","            'correlations': corr_results\n","        },\n","        'barriers': {\n","            'by_impact': barriers_summary,\n","            'by_category': category_summary\n","        }\n","    }\n","\n","if __name__ == \"__main__\":\n","    results = main_part1()"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.patches import Patch\n","import os\n","\n","# Ensure results directory exists\n","os.makedirs(\"results\", exist_ok=True)\n","\n","# -----------------------------------------------\n","# Load Results from Part 1\n","# -----------------------------------------------\n","\n","def load_part1_results():\n","    \"\"\"\n","    Load analysis results from part 1\n","    \"\"\"\n","    print(\"Loading results from Part 1...\")\n","\n","    try:\n","        # Load performance analysis results\n","        performance_results = pd.read_pickle('results/performance_analysis_results.pkl')\n","        avg_performance = performance_results['avg_performance']\n","        improvements = performance_results['improvements']\n","        stat_results = performance_results['stat_results']\n","        corr_results = performance_results['corr_results']\n","\n","        # Load barriers analysis results\n","        barriers_results = pd.read_pickle('results/barriers_analysis_results.pkl')\n","        barriers_data = barriers_results['barriers_by_impact']\n","        category_summary = barriers_results['category_summary']\n","\n","        print(\"Successfully loaded results from Part 1\")\n","\n","        return {\n","            'performance': {\n","                'avg_performance': avg_performance,\n","                'improvements': improvements,\n","                'stat_results': stat_results,\n","                'corr_results': corr_results\n","            },\n","            'barriers': {\n","                'barriers_data': barriers_data,\n","                'category_summary': category_summary\n","            }\n","        }\n","\n","    except FileNotFoundError:\n","        print(\"Error: Could not find Part 1 results. Please run Part 1 first.\")\n","        return None\n","\n","# -----------------------------------------------\n","# Visualization Functions\n","# -----------------------------------------------\n","\n","def create_performance_visualizations(avg_performance, improvements):\n","    \"\"\"\n","    Create visualizations of performance data\n","    \"\"\"\n","    print(\"Creating performance visualizations...\")\n","\n","    # Set plotting style - using updated style name\n","    # Fix for deprecated 'seaborn-whitegrid'\n","    sns.set_theme(style=\"whitegrid\")  # Updated styling approach\n","    sns.set_palette('viridis')\n","\n","    # Raw performance metrics visualization\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","\n","    # Task completion time\n","    sns.barplot(\n","        x='algorithm_category',\n","        y='task_completion_time_ms',\n","        data=avg_performance,\n","        ax=axes[0, 0]\n","    )\n","    axes[0, 0].set_title('Task Completion Time (ms)')\n","    axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45, ha='right')\n","\n","    # Resource utilization\n","    sns.barplot(\n","        x='algorithm_category',\n","        y='resource_utilization_percent',\n","        data=avg_performance,\n","        ax=axes[0, 1]\n","    )\n","    axes[0, 1].set_title('Resource Utilization (%)')\n","    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45, ha='right')\n","\n","    # Energy efficiency\n","    sns.barplot(\n","        x='algorithm_category',\n","        y='energy_efficiency_score',\n","        data=avg_performance,\n","        ax=axes[0, 2]\n","    )\n","    axes[0, 2].set_title('Energy Efficiency Score')\n","    axes[0, 2].set_xticklabels(axes[0, 2].get_xticklabels(), rotation=45, ha='right')\n","\n","    # Failure rate\n","    sns.barplot(\n","        x='algorithm_category',\n","        y='failure_rate_percent',\n","        data=avg_performance,\n","        ax=axes[1, 0]\n","    )\n","    axes[1, 0].set_title('Failure Rate (%)')\n","    axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45, ha='right')\n","\n","    # Adaptation time\n","    sns.barplot(\n","        x='algorithm_category',\n","        y='adaptation_time_ms',\n","        data=avg_performance,\n","        ax=axes[1, 1]\n","    )\n","    axes[1, 1].set_title('Adaptation Time (ms)')\n","    axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n","\n","    plt.tight_layout()\n","    plt.savefig('results/performance_metrics.png', dpi=300)\n","    print(f\"Performance metrics visualization saved to 'results/performance_metrics.png'\")\n","    plt.close()\n","\n","    # Improvement percentage visualization\n","    # Melt the data for easier plotting\n","    melted_data = pd.melt(\n","        improvements,\n","        id_vars=['algorithm_category'],\n","        value_vars=[\n","            'task_completion_time_improvement',\n","            'resource_utilization_improvement',\n","            'energy_efficiency_improvement',\n","            'failure_reduction',\n","            'adaptation_time_improvement'\n","        ],\n","        var_name='metric',\n","        value_name='percent_improvement'\n","    )\n","\n","    # Create the plot\n","    plt.figure(figsize=(14, 8))\n","    sns.barplot(\n","        x='metric',\n","        y='percent_improvement',\n","        hue='algorithm_category',\n","        data=melted_data\n","    )\n","\n","    plt.title('Improvement Percentages by Algorithm Type')\n","    plt.xlabel('Performance Metric')\n","    plt.ylabel('Percent Improvement Over Conventional')\n","    plt.xticks(rotation=45, ha='right')\n","    plt.legend(title='Algorithm Type')\n","    plt.tight_layout()\n","\n","    plt.savefig('results/improvement_percentages.png', dpi=300)\n","    print(f\"Improvement percentages visualization saved to 'results/improvement_percentages.png'\")\n","    plt.close()\n","\n","def create_barrier_visualizations(barriers_data, category_summary):\n","    \"\"\"\n","    Create visualizations of barrier data\n","    \"\"\"\n","    print(\"Creating barrier visualizations...\")\n","\n","    # Set plotting style - using updated style name\n","    sns.set_theme(style=\"whitegrid\")  # Updated styling approach\n","\n","    # Top barriers by impact\n","    plt.figure(figsize=(12, 8))\n","\n","    # Plot bars colored by category\n","    category_colors = {'Technical': 'blue', 'Organizational': 'green', 'Contextual': 'orange'}\n","\n","    # Get top 10 barriers\n","    top_barriers = barriers_data.head(10)\n","\n","    sns.barplot(\n","        x='impact_score',\n","        y='barrier_name',\n","        data=top_barriers,\n","        palette=[category_colors[cat] for cat in top_barriers['barrier_category']]\n","    )\n","\n","    plt.title('Top 10 Implementation Barriers by Impact Score')\n","    plt.xlabel('Impact Score (Frequency × Severity)')\n","    plt.ylabel('')\n","\n","    # Add a legend\n","    legend_elements = [\n","        Patch(facecolor=color, label=cat)\n","        for cat, color in category_colors.items()\n","    ]\n","    plt.legend(handles=legend_elements, title='Barrier Category')\n","\n","    plt.tight_layout()\n","    plt.savefig('results/top_barriers_by_impact.png', dpi=300)\n","    print(f\"Top barriers visualization saved to 'results/top_barriers_by_impact.png'\")\n","    plt.close()\n","\n","    # Barrier category summary\n","    plt.figure(figsize=(10, 6))\n","    sns.barplot(\n","        x='barrier_category',\n","        y='impact_score',\n","        data=category_summary\n","    )\n","    plt.title('Average Barrier Impact by Category')\n","    plt.xlabel('Barrier Category')\n","    plt.ylabel('Average Impact Score')\n","\n","    plt.tight_layout()\n","    plt.savefig('results/barrier_category_impact.png', dpi=300)\n","    print(f\"Barrier category impact visualization saved to 'results/barrier_category_impact.png'\")\n","    plt.close()\n","\n","    # Create barrier heatmap based on severity and frequency\n","    try:\n","        # Create a pivot table of barriers by category and frequency\n","        heatmap_data = pd.pivot_table(\n","            barriers_data,\n","            values='severity_rating',\n","            index='barrier_category',\n","            columns='barrier_name',\n","            aggfunc='mean'\n","        )\n","\n","        plt.figure(figsize=(14, 6))\n","        sns.heatmap(\n","            heatmap_data,\n","            annot=True,\n","            cmap='YlOrRd',\n","            linewidths=.5,\n","            fmt='.1f'\n","        )\n","        plt.title('Barrier Severity by Category and Type')\n","        plt.tight_layout()\n","        plt.savefig('results/barrier_severity_heatmap.png', dpi=300)\n","        print(f\"Barrier severity heatmap saved to 'results/barrier_severity_heatmap.png'\")\n","        plt.close()\n","    except:\n","        print(\"Warning: Could not create barrier heatmap, possibly due to data structure\")\n","\n","# -----------------------------------------------\n","# Framework Development Functions\n","# -----------------------------------------------\n","\n","def create_algorithm_selection_matrix(performance_data):\n","    \"\"\"\n","    Create algorithm selection matrix based on performance analysis\n","    \"\"\"\n","    print(\"Creating algorithm selection matrix...\")\n","\n","    # This would be expanded in a full implementation\n","    application_requirements = [\n","        \"Time-Sensitive Operations\",\n","        \"Resource Optimization\",\n","        \"Multiple Robot Coordination\",\n","        \"Variable Connectivity\",\n","        \"Maximum Overall Performance\"\n","    ]\n","\n","    # Based on performance analysis\n","    recommended_algorithms = [\n","        \"Distributed Edge\",\n","        \"Centralized Cloud\",\n","        \"Federated Learning\",\n","        \"Hybrid with Edge Priority\",\n","        \"Hybrid Cloud-Edge\"\n","    ]\n","\n","    # Create matrix\n","    selection_matrix = pd.DataFrame({\n","        'application_requirement': application_requirements,\n","        'recommended_algorithm': recommended_algorithms\n","    })\n","\n","    selection_matrix.to_csv('results/algorithm_selection_matrix.csv', index=False)\n","    print(f\"Algorithm selection matrix saved to 'results/algorithm_selection_matrix.csv'\")\n","\n","    return selection_matrix\n","\n","def create_barrier_mitigation_pathways(barriers_data):\n","    \"\"\"\n","    Create barrier mitigation pathways based on barrier analysis\n","    \"\"\"\n","    print(\"Creating barrier mitigation pathways...\")\n","\n","    # Get top barriers\n","    top_barriers = barriers_data.head(6)\n","\n","    # Mitigation strategies based on barrier type\n","    mitigation_strategies = [\n","        \"Middleware Solutions + Standardized APIs\",\n","        \"Edge-prioritization for Time-sensitive Functions\",\n","        \"Federated Processing + Encryption Protocols\",\n","        \"Phased Implementation + ROI Prioritization\",\n","        \"Strategic Partnerships + Training Programs\",\n","        \"Compliance-first Design Approach\"\n","    ]\n","\n","    # Create pathways\n","    mitigation_pathways = pd.DataFrame({\n","        'barrier': top_barriers['barrier_name'].tolist(),\n","        'category': top_barriers['barrier_category'].tolist(),\n","        'impact_score': top_barriers['impact_score'].tolist(),\n","        'mitigation_strategy': mitigation_strategies\n","    })\n","\n","    mitigation_pathways.to_csv('results/barrier_mitigation_pathways.csv', index=False)\n","    print(f\"Barrier mitigation pathways saved to 'results/barrier_mitigation_pathways.csv'\")\n","\n","    return mitigation_pathways\n","\n","def create_implementation_framework(performance_data, barriers_data):\n","    \"\"\"\n","    Create comprehensive implementation framework\n","    \"\"\"\n","    print(\"Creating implementation framework...\")\n","\n","    # Algorithm Selection Matrix component\n","    algorithm_selection = create_algorithm_selection_matrix(performance_data)\n","\n","    # Barrier Mitigation Pathways component\n","    barrier_mitigation = create_barrier_mitigation_pathways(barriers_data)\n","\n","    # Create implementation pathway model\n","    implementation_stages = [\n","        \"Initial Focus\",\n","        \"Expanded Implementation\",\n","        \"Advanced Integration\",\n","        \"Comprehensive Optimization\"\n","    ]\n","\n","    focus_areas = [\n","        \"High-impact processes with minimal barriers\",\n","        \"Extended scope with capability development\",\n","        \"Sophisticated algorithms with custom integration\",\n","        \"System-wide optimization with continuous improvement\"\n","    ]\n","\n","    expected_gains = [\n","        \"20-30% improvement in targeted areas\",\n","        \"30-50% improvement across expanded scope\",\n","        \"50-60% improvement with advanced features\",\n","        \"60-75% improvement across all operations\"\n","    ]\n","\n","    implementation_pathway = pd.DataFrame({\n","        'stage': implementation_stages,\n","        'focus_area': focus_areas,\n","        'expected_gains': expected_gains\n","    })\n","\n","    implementation_pathway.to_csv('results/implementation_pathway.csv', index=False)\n","    print(f\"Implementation pathway saved to 'results/implementation_pathway.csv'\")\n","\n","    # Create contextual adaptation matrix\n","    contexts = [\n","        \"Manufacturing\",\n","        \"Healthcare\",\n","        \"Logistics\",\n","        \"Retail\",\n","        \"Finance\"\n","    ]\n","\n","    primary_focus = [\n","        \"Reliability, Integration\",\n","        \"Compliance, Security\",\n","        \"Responsiveness, Scale\",\n","        \"Cost, Flexibility\",\n","        \"Security, Compliance\"\n","    ]\n","\n","    recommended_approach = [\n","        \"Hybrid with Edge Priority\",\n","        \"Federated Learning\",\n","        \"Distributed Edge\",\n","        \"Centralized Cloud\",\n","        \"Hybrid with Isolation\"\n","    ]\n","\n","    contextual_adaptation = pd.DataFrame({\n","        'industry': contexts,\n","        'primary_focus': primary_focus,\n","        'recommended_approach': recommended_approach\n","    })\n","\n","    contextual_adaptation.to_csv('results/contextual_adaptation.csv', index=False)\n","    print(f\"Contextual adaptation matrix saved to 'results/contextual_adaptation.csv'\")\n","\n","    # Create final framework document\n","    framework_document = {\n","        'algorithm_selection': algorithm_selection,\n","        'barrier_mitigation': barrier_mitigation,\n","        'implementation_pathway': implementation_pathway,\n","        'contextual_adaptation': contextual_adaptation\n","    }\n","\n","    pd.to_pickle(framework_document, 'results/implementation_framework.pkl')\n","    print(f\"Complete implementation framework saved to 'results/implementation_framework.pkl'\")\n","\n","    # Create a text summary\n","    with open('results/implementation_framework_summary.txt', 'w') as f:\n","        f.write(\"# Cloud Robotics Implementation Framework\\n\\n\")\n","        f.write(\"## Algorithm Selection Matrix\\n\")\n","        f.write(algorithm_selection.to_string(index=False))\n","        f.write(\"\\n\\n## Barrier Mitigation Pathways\\n\")\n","        f.write(barrier_mitigation.to_string(index=False))\n","        f.write(\"\\n\\n## Implementation Pathway\\n\")\n","        f.write(implementation_pathway.to_string(index=False))\n","        f.write(\"\\n\\n## Contextual Adaptation Matrix\\n\")\n","        f.write(contextual_adaptation.to_string(index=False))\n","\n","    print(f\"Framework summary saved to 'results/implementation_framework_summary.txt'\")\n","\n","    return framework_document\n","\n","# -----------------------------------------------\n","# Main Execution for Part 2\n","# -----------------------------------------------\n","\n","def main_part2():\n","    \"\"\"\n","    Main execution function for part 2\n","    \"\"\"\n","    print(\"Starting Cloud Robotics Analysis - Part 2 (Visualizations and Framework)...\")\n","\n","    # Load results from part 1\n","    results = load_part1_results()\n","\n","    if results is None:\n","        print(\"Error: Cannot proceed without Part 1 results.\")\n","        return None\n","\n","    # Extract data\n","    avg_performance = results['performance']['avg_performance']\n","    improvements = results['performance']['improvements']\n","    barriers_data = results['barriers']['barriers_data']\n","    category_summary = results['barriers']['category_summary']\n","\n","    # Create visualizations\n","    create_performance_visualizations(avg_performance, improvements)\n","    create_barrier_visualizations(barriers_data, category_summary)\n","\n","    # Create implementation framework\n","    framework = create_implementation_framework(avg_performance, barriers_data)\n","\n","    print(\"\\nPart 2 complete! Visualizations and implementation framework created.\")\n","    print(\"All results are available in the 'results' directory.\")\n","\n","    return {\n","        'visualizations': {\n","            'performance': 'results/performance_metrics.png',\n","            'improvements': 'results/improvement_percentages.png',\n","            'barriers': 'results/top_barriers_by_impact.png',\n","            'categories': 'results/barrier_category_impact.png'\n","        },\n","        'framework': framework\n","    }\n","\n","if __name__ == \"__main__\":\n","    results = main_part2()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAIJ3JDZa7yu","executionInfo":{"status":"ok","timestamp":1743193709758,"user_tz":0,"elapsed":5337,"user":{"displayName":"Imo Enang","userId":"02918714309836997024"}},"outputId":"206da47f-ba9a-48b0-dd6d-68afa0235cf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Cloud Robotics Analysis - Part 2 (Visualizations and Framework)...\n","Loading results from Part 1...\n","Successfully loaded results from Part 1\n","Creating performance visualizations...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-9e66c3fe1cf9>:79: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45, ha='right')\n","<ipython-input-7-9e66c3fe1cf9>:89: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45, ha='right')\n","<ipython-input-7-9e66c3fe1cf9>:99: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  axes[0, 2].set_xticklabels(axes[0, 2].get_xticklabels(), rotation=45, ha='right')\n","<ipython-input-7-9e66c3fe1cf9>:109: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45, ha='right')\n","<ipython-input-7-9e66c3fe1cf9>:119: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n"]},{"output_type":"stream","name":"stdout","text":["Performance metrics visualization saved to 'results/performance_metrics.png'\n","Improvement percentages visualization saved to 'results/improvement_percentages.png'\n","Creating barrier visualizations...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-9e66c3fe1cf9>:180: FutureWarning: \n","\n","Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n","\n","  sns.barplot(\n","<ipython-input-7-9e66c3fe1cf9>:180: UserWarning: The palette list has more values (10) than needed (1), which may not be intended.\n","  sns.barplot(\n"]},{"output_type":"stream","name":"stdout","text":["Top barriers visualization saved to 'results/top_barriers_by_impact.png'\n","Barrier category impact visualization saved to 'results/barrier_category_impact.png'\n","Barrier severity heatmap saved to 'results/barrier_severity_heatmap.png'\n","Creating implementation framework...\n","Creating algorithm selection matrix...\n","Algorithm selection matrix saved to 'results/algorithm_selection_matrix.csv'\n","Creating barrier mitigation pathways...\n","Barrier mitigation pathways saved to 'results/barrier_mitigation_pathways.csv'\n","Implementation pathway saved to 'results/implementation_pathway.csv'\n","Contextual adaptation matrix saved to 'results/contextual_adaptation.csv'\n","Complete implementation framework saved to 'results/implementation_framework.pkl'\n","Framework summary saved to 'results/implementation_framework_summary.txt'\n","\n","Part 2 complete! Visualizations and implementation framework created.\n","All results are available in the 'results' directory.\n"]}]}]}